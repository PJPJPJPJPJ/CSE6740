{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _ISYE 6740 Homework 4_\n",
    "## _Name_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SVM (30 points)\n",
    "> (a) (10 points) Explain why can we set the margin $c = 1$ to derive the SVM formulation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "> First, we know that $ w^t x + b = c$, and therefore, when c scales, $ w^t x + b$ scales simultaneously too. So the scaling factor of $c$ will eventually be cancelled out in the $$ min \\frac{||\\mathbf{w^t x + b}||}{||\\mathbf{w}||}$$\\\n",
    "Second, when we are trying to $$ min \\frac{||\\mathbf{w^t x + b}||}{||\\mathbf{w}||}$$ we know that for the numerator: $min{||\\mathbf{w^t x + b}||}$ we are essentially looking for the point(s) lying on the data boundary to hold the support vector. And no matter what the $c$ is, we are always going to be able to find at least one point. That means the numerator is not relavant in this minimization process, and we can purely focus on $min ||w||$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (b) (10 points) Using Lagrangian dual formulation, show that the weight vector can be represented as\n",
    "$$w = \\sum_{i=1}^n \\alpha_i y_i x_i.$$\n",
    "where $\\alpha_i \\geq 0$ are the dual variables. What does this imply in terms of how to relate data to $w$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    ">The optimal $w$ is expressed in the form of the product of dual variables, the response variable, and the feature data. This implies that when plug in the $w$ back into the formula for the support vectors, we are going to get the inner product form of the original dataset, multiplied by the dual variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (c) (10 points) Explain why only the data points on the ``margin'' will contribute to the sum above, i.e., playing a role in defining $w$. Hint: use the Lagrangian multiplier derivation and KKT condition we discussed in class. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    ">According to the KKT conditions, we have to satisfy the complimentary slackness, which means that the product of the dual variable and the inequality constraint cannot be zero at the same time. At the optimal solution, the slack terms, in this case, the inequality terms will need to equal to zero in order to make sure that the primal and the dual feasibility holds, that is their optimal values are the same. \\\n",
    "\\\n",
    "Based on this, we know that only the points on the support vectors are the optimal solutions, and thus, the $w^T x + b - c$ term will equal to zero, and in order to make the whole term zero, we need to make $\\alpha$ a positive number. Whereas the points not on the support vectors will have $w^T x + b - c \\geq 0$, and thus we will need to make their $\\alpha$ to zero.\\\n",
    "\\\n",
    "Going back to the sum term above, because we have the points on the support vector with non-zero $\\alpha$s, and the other points with $\\alpha = 0$, only those points on the boundary will contribute to the summation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple SVM by hand. (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we only have four training examples in two dimensions as shown in Fig. The positive samples at $x_1 = (0, 0)$, $x_2 = (2, 2)$ and negative samples at $x_3 = (h, 1)$ and $x_4 = (0, 3)$. \n",
    "\n",
    "<table><tr><td><img src='svm.png'></td></tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (a) (10 points) For what range of parameter $h > 0$, the training points are still linearly separable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "> First, we can clearly see that the class boundary for the blue class is $ y = x$. And since $h>0$, so even at its limit, that is, for point $(0,1)$, we still have its distance to the blue class of $\\frac{\\sqrt{2}}{2}$, whereas point $(0,3)$ has a distance to the blue point of $\\frac{3\\sqrt{2}}{2}$.\\\n",
    "\\\n",
    "Since at $h$'s boundary condition, the point's distance to the blue class is still less than the point $(0,3)$, this means that the point $(h,1)$ will be the only possible point that touches or cross the boundary of the blue class. In this case, the boundary condition for $h$ is when $h = 1$. Therefore, in order for the red class to be separated from the blue, $ 0<h<1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (b) (10 points) Does the orientation of the maximum margin decision boundary change as $h$ changes, when the points are separable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "> Although the above mentioned class boundary is not the decision boundary, but we can infer from the blue class that in order for the blue class to be separable, the support vector on the blue side will either have its $w < 1$ when it only passes through point $(0,0)$, or have its $w>1$ when it only passes through point $(2,2)$. Either way, the support vector will renders the decision boundary to move even closer to point $(h,1)$ before it reaches point $(0,3)$, as we know the decision boundary and the support vectors are parallel lines.\\\n",
    "\\\n",
    "What this implies is that, either way, point$(h,1)$ will be the point to hold the support vector of the red class. \\\n",
    "\\\n",
    "In the meantime, as is explained above, all the other possible support vectors for the blue class except $y = x$ will render a shorter distance to the red class, which is not what we want, because in SVM, we want to maximize the class gap. Therefore, $y = x$, as we know it, is the furthest the support vector it can go before it becomes unable to separate the two classes, and will also always be the support vector for the blue class. Thus, we can naturally derive that the decision boundary will also have $w = 1$ unchanged regardless of $h$ even if we know point$(h,1)$ will be the point that holds the support vector on the red side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Neural Networks (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (a) (10 points)\n",
    "Consider a neural networks for a binary classification using sigmoid function for each unit. If the network has no hidden layer, explain why the model is equivalent to logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "> In neural networks, we are trying to $$min\\; \\ell(w, \\alpha, \\beta) = \\sum_{i=1}^m (y^i - \\sigma(w^T z^i))^2$$\n",
    "where $z^i_1 = \\sigma(\\alpha^T z^i)$ and $z^i_2 = \\sigma(\\beta^T z^i)$ from the previous iterations, and $\\alpha$ and $\\beta$ are the weights for the different logistic regression models. \\\n",
    "\\\n",
    "When we do not have a hidden layer, this means that we no longer fit the data using different parameters of the logistic regression model and combine their weighed results together. \\\n",
    "\\\n",
    "Therefore, we don't subsititue $\\sigma(\\beta^T z^i)$ with the results from previous rounds, and directly minimize the $l$ function. \n",
    "\\begin{align}\n",
    "\\min\\; \\ell(w, \\alpha, \\beta) = \\sum_{i=1}^m (y^i - \\sigma(w^T x^i))^2\\\\\n",
    "\\ =\\sum_{i=1}^m (y^i)^2 + \\sigma(w^T x^i)^2 - 2y^i\\sigma(w^T x^i)\\\\\n",
    "\\ = \\sum_{i=1}^p (y^i)^2 + \\sigma(w^T x^i)^2 - 2\\sigma(w^T x^i)\\\\ \n",
    "\\ = \\sum_{i=1}^p (\\sigma(w^T x^i) - 1)^2 -1\n",
    "\\end{align}\n",
    "\\\n",
    "The $p$ stands for the data which has a response $y^i$ of 1. Those whose response is 0 are left out of the equation since their product with the sigmoid function will still be 0.\\\n",
    "\\\n",
    "Taking the derivative for the above formula, we get\n",
    "$$l' = 2(\\sigma(w^T z^i) - 1)$$\n",
    "and since $\\sigma$ will always be between 0 and 1, therefore the above derivative will always be negative, meaning that the original $l$ function is decreasing monotonically.\n",
    "\\begin{align}\n",
    "\\min\\; g = \\sum_{i=1}^p (\\sigma(w^T z^i) - 1)^2 -1\\\\\n",
    "\\ = min\\; \\sum_{i=1}^p \\frac{\\exp^2}{(1+\\exp)^2} -1\\\\\n",
    "\\ = min\\; \\sum_{i=1}^p - \\frac{2\\exp + 1}{(1+\\exp)^2}\\\\\n",
    "\\ = - max\\; \\sum_{i=1}^p \\frac{2\\exp + 1}{(1+\\exp)^2}\\\\\n",
    "\\end{align}\n",
    "Getting the derivation of the above $g$ function, we get\n",
    "\\begin{align}\n",
    "\\ g' = \\sum_{i=1}^p \\frac{2\\exp}{(1+\\exp)^2} - \\frac{2\\exp(2\\exp+1)}{(1+\\exp)^3}\\\\\n",
    "\\ = \\sum_{i=1}^p \\frac{-2\\exp^2}{(1+\\exp)^3} \\leq 0\\\\\n",
    "\\end{align}\n",
    "So this $$max \\;g = \\sum_{i=1}^p \\frac{2\\exp + 1}{(1+\\exp)^2}$$ function is a monotonically decreasing function, and we can for sure obtain a maximum value at a given range. This is esentially equivalent to the maximum log-likelihood function used in the pure logistic regression solver, although the equation looks different, but in the end, we are looking for the same goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (b) (10 points) \n",
    "Consider a simple two-layer network in the lecture slides. Given $m$ training data $(x^i, y^i)$, $i = 1, \\ldots, m$, the cost function used to training the neural networks\n",
    "$$\\ell(w, \\alpha, \\beta) = \\sum_{i=1}^m (y^i - \\sigma(w^T z^i))^2$$\n",
    "\n",
    ">where $\\sigma (x) = 1/(1+e^{-x})$ is the sigmoid function, $z^i$ is a two-dimensional vector such that  $z_1^i = \\sigma(\\alpha^T x^i)$, and $z_2^i = \\sigma(\\beta^T x^i)$. Show the that the gradient is given by\n",
    "$$\\frac{\\partial \\ell(w, \\alpha, \\beta) }{\\partial w}\n",
    "= - \\sum_{i=1}^m 2(y^i - \\sigma(u^i))\\sigma(u^i)(1-\\sigma(u^i)) z^i,\n",
    "$$\n",
    "where $u^i = w^T z^i$. Also find the gradient of $\\ell(w, \\alpha, \\beta)$ with respect to $\\alpha$ and $\\beta$ and write down their expression.\n",
    "\\end{enumerate}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer\n",
    "\n",
    ">\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\ell(w, \\alpha, \\beta) }{\\partial w} &= - \\sum_{i=1}^m 2(y^i - \\sigma(w^T z^i))[\\sigma(w^T z^i)]'(w^T z^i)'\n",
    "&= - \\sum_{i=1}^m 2(y^i - \\sigma(u^i))\\sigma(u^i)(1-\\sigma(u^i)) z^i\n",
    "\\end{align}\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\ell(w, \\alpha, \\beta) }{\\partial \\alpha} &= - \\sum_{i=1}^m 2(y^i - \\sigma(w^T z^i))[\\sigma(w^T z^i)]'(w^T z^i)'\\\\\n",
    "&= - \\sum_{i=1}^m 2(y^i - \\sigma(u^i))\\sigma(u^i)(1-\\sigma(u^i)) z^i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comparing SVM and simple neural networks (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This question is to implement and compare **SVM and simple neural networks** for the same datasets we tried for the last homework (so in the end, we have compared 5 classification algorithms on two datasets). We suggest to use *Scikit-learn*, which is a commonly-used and powerful *Python* library with various machine learning tools. But you can also use other similar libraries in other programming languages of your choice to perform the tasks. \n",
    "\n",
    "You may use a neural networks function **sklearn.neural\\_network** with **hidden\\_layer\\_sizes=(5, 2)**. Tune the step size so you have reasonable results. You may use **svc** and tune the penalty term $C$ to get reasonable results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part One (Divorce classification/prediction) (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare using the same dataset as the last homework, which is about participants who completed the personal information form and a divorce predictors scale. \n",
    "\n",
    "The data is a modified version of the publicly available at \\url{https://archive.ics.uci.edu/ml/datasets/Divorce+Predictors+data+set} (by injecting noise so you will not replicate the results on uci website). There are 170 participants and 54 attributes (or predictor variables) that are all real-valued. The dataset was the same as the last homework. The last column of the CSV file is label $y$ (1 means \\``divorce'', 0 means \\``no divorce''). Each column is for one feature (predictor variable), and each row is a sample (participant). A detailed explanation for each feature (predictor variable) can be found at the website link above. Our goal is to build a classifier using training data, such that given a test sample, we can classify (or essentially predict) whether its label is 0 (\\``no divorce'') or 1 (``divorce''). \n",
    "\n",
    "Build two classifiers using SVM and a simple neural networks. First random shuffle the data set. Then use the first $80\\%$ data for training and the remaining $20\\%$ for testing. If you use **scikit-learn** you can use **train\\_test\\_split** to split the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (a) (10 points) Report testing accuracy for each of the two classifiers.  Comment on their performance: which performs better and make a guess why it performs better in this setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.560903</td>\n",
       "      <td>3.681587</td>\n",
       "      <td>3.450467</td>\n",
       "      <td>3.211998</td>\n",
       "      <td>-1.203045</td>\n",
       "      <td>0.597706</td>\n",
       "      <td>-0.970093</td>\n",
       "      <td>-0.750970</td>\n",
       "      <td>-0.511495</td>\n",
       "      <td>-0.133660</td>\n",
       "      <td>...</td>\n",
       "      <td>2.077401</td>\n",
       "      <td>1.184182</td>\n",
       "      <td>3.955069</td>\n",
       "      <td>2.608046</td>\n",
       "      <td>2.303629</td>\n",
       "      <td>1.721660</td>\n",
       "      <td>3.275018</td>\n",
       "      <td>1.761019</td>\n",
       "      <td>1.215237</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.153272</td>\n",
       "      <td>5.173858</td>\n",
       "      <td>4.100690</td>\n",
       "      <td>2.580173</td>\n",
       "      <td>3.305788</td>\n",
       "      <td>-1.505512</td>\n",
       "      <td>-0.029398</td>\n",
       "      <td>5.702657</td>\n",
       "      <td>2.230281</td>\n",
       "      <td>4.975496</td>\n",
       "      <td>...</td>\n",
       "      <td>3.467076</td>\n",
       "      <td>2.451984</td>\n",
       "      <td>3.504294</td>\n",
       "      <td>5.324240</td>\n",
       "      <td>4.480607</td>\n",
       "      <td>5.375248</td>\n",
       "      <td>2.270379</td>\n",
       "      <td>2.167944</td>\n",
       "      <td>2.191214</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.226241</td>\n",
       "      <td>1.575322</td>\n",
       "      <td>2.389117</td>\n",
       "      <td>2.725405</td>\n",
       "      <td>-0.304562</td>\n",
       "      <td>2.832803</td>\n",
       "      <td>1.787779</td>\n",
       "      <td>0.565755</td>\n",
       "      <td>1.328212</td>\n",
       "      <td>2.335353</td>\n",
       "      <td>...</td>\n",
       "      <td>1.200917</td>\n",
       "      <td>1.241794</td>\n",
       "      <td>2.207492</td>\n",
       "      <td>1.228034</td>\n",
       "      <td>0.870052</td>\n",
       "      <td>1.685040</td>\n",
       "      <td>2.341985</td>\n",
       "      <td>-0.444320</td>\n",
       "      <td>2.527452</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.553458</td>\n",
       "      <td>2.859042</td>\n",
       "      <td>2.928414</td>\n",
       "      <td>1.833241</td>\n",
       "      <td>1.271119</td>\n",
       "      <td>4.165213</td>\n",
       "      <td>2.078597</td>\n",
       "      <td>4.506175</td>\n",
       "      <td>2.521628</td>\n",
       "      <td>2.747315</td>\n",
       "      <td>...</td>\n",
       "      <td>3.196291</td>\n",
       "      <td>2.204824</td>\n",
       "      <td>3.664982</td>\n",
       "      <td>3.689508</td>\n",
       "      <td>2.577677</td>\n",
       "      <td>3.171884</td>\n",
       "      <td>2.164660</td>\n",
       "      <td>1.813024</td>\n",
       "      <td>1.376033</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.506547</td>\n",
       "      <td>1.419223</td>\n",
       "      <td>1.716153</td>\n",
       "      <td>1.319274</td>\n",
       "      <td>2.853840</td>\n",
       "      <td>0.047412</td>\n",
       "      <td>-0.016515</td>\n",
       "      <td>0.620795</td>\n",
       "      <td>1.202992</td>\n",
       "      <td>0.078347</td>\n",
       "      <td>...</td>\n",
       "      <td>1.806657</td>\n",
       "      <td>2.085539</td>\n",
       "      <td>2.012551</td>\n",
       "      <td>1.899477</td>\n",
       "      <td>1.510134</td>\n",
       "      <td>1.373350</td>\n",
       "      <td>2.551119</td>\n",
       "      <td>0.846321</td>\n",
       "      <td>-0.066858</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>-1.845462</td>\n",
       "      <td>-0.808531</td>\n",
       "      <td>0.309481</td>\n",
       "      <td>-0.979520</td>\n",
       "      <td>-0.556496</td>\n",
       "      <td>-0.395338</td>\n",
       "      <td>-0.502979</td>\n",
       "      <td>-0.798474</td>\n",
       "      <td>-1.473114</td>\n",
       "      <td>2.428689</td>\n",
       "      <td>...</td>\n",
       "      <td>1.021530</td>\n",
       "      <td>-0.108714</td>\n",
       "      <td>2.308311</td>\n",
       "      <td>-0.287032</td>\n",
       "      <td>0.046402</td>\n",
       "      <td>4.702990</td>\n",
       "      <td>4.814076</td>\n",
       "      <td>2.510736</td>\n",
       "      <td>2.225078</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.337077</td>\n",
       "      <td>-0.483750</td>\n",
       "      <td>0.823365</td>\n",
       "      <td>0.563178</td>\n",
       "      <td>-1.270118</td>\n",
       "      <td>0.978814</td>\n",
       "      <td>-1.027141</td>\n",
       "      <td>-0.581733</td>\n",
       "      <td>-1.338374</td>\n",
       "      <td>0.463138</td>\n",
       "      <td>...</td>\n",
       "      <td>3.182008</td>\n",
       "      <td>3.265522</td>\n",
       "      <td>2.005206</td>\n",
       "      <td>1.943546</td>\n",
       "      <td>4.070406</td>\n",
       "      <td>2.555771</td>\n",
       "      <td>4.383673</td>\n",
       "      <td>1.294633</td>\n",
       "      <td>2.477147</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>2.304462</td>\n",
       "      <td>1.844857</td>\n",
       "      <td>-0.095230</td>\n",
       "      <td>-0.405619</td>\n",
       "      <td>1.179862</td>\n",
       "      <td>1.226855</td>\n",
       "      <td>1.384235</td>\n",
       "      <td>-0.206427</td>\n",
       "      <td>1.973713</td>\n",
       "      <td>1.940605</td>\n",
       "      <td>...</td>\n",
       "      <td>1.907090</td>\n",
       "      <td>0.239489</td>\n",
       "      <td>1.249397</td>\n",
       "      <td>-0.077476</td>\n",
       "      <td>1.429052</td>\n",
       "      <td>-0.067223</td>\n",
       "      <td>5.132404</td>\n",
       "      <td>-0.871550</td>\n",
       "      <td>-0.637281</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>0.369439</td>\n",
       "      <td>0.063279</td>\n",
       "      <td>-0.466090</td>\n",
       "      <td>0.017346</td>\n",
       "      <td>-1.197056</td>\n",
       "      <td>0.038995</td>\n",
       "      <td>1.735441</td>\n",
       "      <td>1.037408</td>\n",
       "      <td>0.183137</td>\n",
       "      <td>0.333348</td>\n",
       "      <td>...</td>\n",
       "      <td>2.893040</td>\n",
       "      <td>4.305077</td>\n",
       "      <td>4.685423</td>\n",
       "      <td>1.695163</td>\n",
       "      <td>1.297874</td>\n",
       "      <td>2.588636</td>\n",
       "      <td>4.544230</td>\n",
       "      <td>3.209139</td>\n",
       "      <td>0.094538</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>-1.353336</td>\n",
       "      <td>-0.209975</td>\n",
       "      <td>1.165342</td>\n",
       "      <td>1.371027</td>\n",
       "      <td>0.269725</td>\n",
       "      <td>-0.756154</td>\n",
       "      <td>0.312946</td>\n",
       "      <td>1.649918</td>\n",
       "      <td>1.547073</td>\n",
       "      <td>-0.156679</td>\n",
       "      <td>...</td>\n",
       "      <td>4.479049</td>\n",
       "      <td>4.143191</td>\n",
       "      <td>2.865900</td>\n",
       "      <td>-0.716917</td>\n",
       "      <td>0.867769</td>\n",
       "      <td>3.794449</td>\n",
       "      <td>2.318532</td>\n",
       "      <td>1.358642</td>\n",
       "      <td>-0.406850</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0    1.560903  3.681587  3.450467  3.211998 -1.203045  0.597706 -0.970093   \n",
       "1    4.153272  5.173858  4.100690  2.580173  3.305788 -1.505512 -0.029398   \n",
       "2    2.226241  1.575322  2.389117  2.725405 -0.304562  2.832803  1.787779   \n",
       "3    3.553458  2.859042  2.928414  1.833241  1.271119  4.165213  2.078597   \n",
       "4    0.506547  1.419223  1.716153  1.319274  2.853840  0.047412 -0.016515   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "165 -1.845462 -0.808531  0.309481 -0.979520 -0.556496 -0.395338 -0.502979   \n",
       "166  0.337077 -0.483750  0.823365  0.563178 -1.270118  0.978814 -1.027141   \n",
       "167  2.304462  1.844857 -0.095230 -0.405619  1.179862  1.226855  1.384235   \n",
       "168  0.369439  0.063279 -0.466090  0.017346 -1.197056  0.038995  1.735441   \n",
       "169 -1.353336 -0.209975  1.165342  1.371027  0.269725 -0.756154  0.312946   \n",
       "\n",
       "           7         8         9   ...        45        46        47  \\\n",
       "0   -0.750970 -0.511495 -0.133660  ...  2.077401  1.184182  3.955069   \n",
       "1    5.702657  2.230281  4.975496  ...  3.467076  2.451984  3.504294   \n",
       "2    0.565755  1.328212  2.335353  ...  1.200917  1.241794  2.207492   \n",
       "3    4.506175  2.521628  2.747315  ...  3.196291  2.204824  3.664982   \n",
       "4    0.620795  1.202992  0.078347  ...  1.806657  2.085539  2.012551   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "165 -0.798474 -1.473114  2.428689  ...  1.021530 -0.108714  2.308311   \n",
       "166 -0.581733 -1.338374  0.463138  ...  3.182008  3.265522  2.005206   \n",
       "167 -0.206427  1.973713  1.940605  ...  1.907090  0.239489  1.249397   \n",
       "168  1.037408  0.183137  0.333348  ...  2.893040  4.305077  4.685423   \n",
       "169  1.649918  1.547073 -0.156679  ...  4.479049  4.143191  2.865900   \n",
       "\n",
       "           48        49        50        51        52        53   54  \n",
       "0    2.608046  2.303629  1.721660  3.275018  1.761019  1.215237  1.0  \n",
       "1    5.324240  4.480607  5.375248  2.270379  2.167944  2.191214  1.0  \n",
       "2    1.228034  0.870052  1.685040  2.341985 -0.444320  2.527452  1.0  \n",
       "3    3.689508  2.577677  3.171884  2.164660  1.813024  1.376033  1.0  \n",
       "4    1.899477  1.510134  1.373350  2.551119  0.846321 -0.066858  1.0  \n",
       "..        ...       ...       ...       ...       ...       ...  ...  \n",
       "165 -0.287032  0.046402  4.702990  4.814076  2.510736  2.225078  0.0  \n",
       "166  1.943546  4.070406  2.555771  4.383673  1.294633  2.477147  0.0  \n",
       "167 -0.077476  1.429052 -0.067223  5.132404 -0.871550 -0.637281  0.0  \n",
       "168  1.695163  1.297874  2.588636  4.544230  3.209139  0.094538  0.0  \n",
       "169 -0.716917  0.867769  3.794449  2.318532  1.358642 -0.406850  0.0  \n",
       "\n",
       "[170 rows x 55 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "base_path = os.getcwd()\n",
    "\n",
    "file_path = os.path.join(base_path, \"\", \"marriage.csv\")\n",
    "\n",
    "marriage = pd.read_csv(file_path, sep = \",\",header=None)\n",
    "marriage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import datasets, classifiers and performance metrics\n",
    "from sklearn import datasets, svm, metrics\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split \n",
    "x = marriage.iloc[:,:-1]\n",
    "y = marriage.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = ShuffleSplit(n_splits=5, test_size=.2, random_state=0)\n",
    "\n",
    "indexlist = []\n",
    "\n",
    "for train_index, test_index in rs.split(x):\n",
    "    tempdict = {}\n",
    "    tempdict['train'] = train_index\n",
    "    tempdict['test'] = test_index\n",
    "    indexlist.append(tempdict)\n",
    "\n",
    "\n",
    "#print(indexlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = x.iloc[indexlist[0]['train']]\n",
    "train_response = y.iloc[indexlist[0]['train']]\n",
    "test_data = x.iloc[indexlist[0]['test']]\n",
    "test_response = y.iloc[indexlist[0]['test']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      1.00      0.94        16\n",
      "         1.0       1.00      0.89      0.94        18\n",
      "\n",
      "    accuracy                           0.94        34\n",
      "   macro avg       0.94      0.94      0.94        34\n",
      "weighted avg       0.95      0.94      0.94        34\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[16  0]\n",
      " [ 2 16]]\n"
     ]
    }
   ],
   "source": [
    "classifier = svm.SVC(gamma=0.001)\n",
    "\n",
    "classifier.fit(train_data, train_response)\n",
    "\n",
    "predicted = classifier.predict(test_data)\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(test_response, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(test_response, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
       "       0., 1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifier_nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(20, 5), random_state=1)\n",
    "\n",
    "classifier_nn.fit(train_data, train_response)\n",
    "\n",
    "predict_nn = classifier_nn.predict(test_data)\n",
    "\n",
    "predict_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(20, 5), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=1, shuffle=True, solver='lbfgs',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.94      0.91        16\n",
      "         1.0       0.94      0.89      0.91        18\n",
      "\n",
      "    accuracy                           0.91        34\n",
      "   macro avg       0.91      0.91      0.91        34\n",
      "weighted avg       0.91      0.91      0.91        34\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[15  1]\n",
      " [ 2 16]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier_nn, metrics.classification_report(test_response, predict_nn)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(test_response, predict_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Compare the two classification methods on the divorce data\n",
    "> The result of the SVM and Neural Network shows that SVM performs slightly better. It correctly classifies all the \"1\"s. In this setting where the dimensions are high relative to the number of data points, so using the neural network which assigns weights to the features might have some issues with the linear dependency of each feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (b) (10 points) Use the first two features to train two new classifiers. Plot the data points and decision boundary of each classifier. Comment on the difference between the decision boundary for the two classifiers. Please clearly represent the data points with different labels using different colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = x.iloc[indexlist[0]['train'],:2]\n",
    "train_response = y.iloc[indexlist[0]['train']]\n",
    "test_data = x.iloc[indexlist[0]['test'],:2]\n",
    "test_response = y.iloc[indexlist[0]['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.88      0.82        16\n",
      "         1.0       0.88      0.78      0.82        18\n",
      "\n",
      "    accuracy                           0.82        34\n",
      "   macro avg       0.83      0.83      0.82        34\n",
      "weighted avg       0.83      0.82      0.82        34\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[14  2]\n",
      " [ 4 14]]\n"
     ]
    }
   ],
   "source": [
    "classifier = svm.SVC(gamma=0.001)\n",
    "\n",
    "classifier.fit(train_data, train_response)\n",
    "\n",
    "predicted = classifier.predict(test_data)\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(test_response, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(test_response, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=1, shuffle=True, solver='lbfgs',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.88      0.85        16\n",
      "         1.0       0.88      0.83      0.86        18\n",
      "\n",
      "    accuracy                           0.85        34\n",
      "   macro avg       0.85      0.85      0.85        34\n",
      "weighted avg       0.85      0.85      0.85        34\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[14  2]\n",
      " [ 3 15]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifier_nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "classifier_nn.fit(train_data, train_response)\n",
    "\n",
    "predict_nn = classifier_nn.predict(test_data)\n",
    "\n",
    "predict_nn\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier_nn, metrics.classification_report(test_response, predict_nn)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(test_response, predict_nn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(136, 2)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min, x_max = train_data.to_numpy()[:, 0].min() - 1, train_data.to_numpy()[:, 0].max() + 1\n",
    "y_min, y_max = train_data.to_numpy()[:, 1].min() - 1, train_data.to_numpy()[:, 1].max() + 1\n",
    "h = .02 \n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "\n",
    "titles = ['svm','neural network']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, clf in enumerate((classifier.fit(train_data, train_response), classifier_nn.fit(train_data, train_response))):\n",
    "    plt.subplot(1, 2, i + 1)\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(train_data.to_numpy()[:, 0], train_data.to_numpy()[:, 1], c=train_response, cmap=plt.cm.coolwarm)\n",
    "    plt.xlabel('0')\n",
    "    plt.ylabel('1')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(titles[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis:Compare the two classification boundaries\n",
    "> We can see that the boundary of SVM is a straight line, which is consistent with the concept of Support Vector. \n",
    "> The boundary of the neural network here is a zigzag line, and the reason is because it combines multiple logistic regression models and go through multiple layers. Each layer will renders a weighed combination of logistic regression model that will serve as the data for the next layer. So eventually, the data has been transformed, which is why the boundary do not come in a straight line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part Two (Handwritten digits classification) (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> (10 points) Repeat the above part (a) using the **MNIST Data** in our **Homework 3**. Here, give \\``digit'' 6 label $y = 1$, and give ``digit'' 2 label $y = 0$. All the pixels in each image will be the feature (predictor variables) for that sample (i.e., image). Our goal is to build classifiers such that given a new testing sample, we can tell it is a 2 or a 6. Using the first $80\\%$ of the samples for training and remaining $20\\%$ for testing. Report the classification accuracy on testing data, for each of the two classifiers. Comment on their performance: which performs better and make a guess why they perform better in this setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "raw_data = loadmat(os.path.join(base_path, \"\", \"data.mat\"))\n",
    "\n",
    "images = raw_data['data']\n",
    "images_t = images.transpose()\n",
    "\n",
    "labels = loadmat('C:/Users/pjgab/Desktop/OMA/CSE6740/homework2/hw2/data/label.mat')\n",
    "labels = labels['trueLabel']\n",
    "\n",
    "labels_re = np.where(labels == 2, -1, 1).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train, img_test, digit_train, digit_test = train_test_split(images_t, labels_re, test_size=0.2, random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.98      0.96      0.97       221\n",
      "           1       0.95      0.98      0.96       177\n",
      "\n",
      "    accuracy                           0.97       398\n",
      "   macro avg       0.97      0.97      0.97       398\n",
      "weighted avg       0.97      0.97      0.97       398\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[212   9]\n",
      " [  4 173]]\n"
     ]
    }
   ],
   "source": [
    "classifier = svm.SVC(gamma=0.001)\n",
    "\n",
    "classifier.fit(img_train, digit_train)\n",
    "\n",
    "predicted = classifier.predict(img_test)\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(digit_test, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(digit_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
      "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "              hidden_layer_sizes=(200, 50), learning_rate='constant',\n",
      "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
      "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
      "              power_t=0.5, random_state=10, shuffle=True, solver='lbfgs',\n",
      "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "              warm_start=False):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       1.00      0.98      0.99       221\n",
      "           1       0.98      0.99      0.99       177\n",
      "\n",
      "    accuracy                           0.99       398\n",
      "   macro avg       0.99      0.99      0.99       398\n",
      "weighted avg       0.99      0.99      0.99       398\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[217   4]\n",
      " [  1 176]]\n"
     ]
    }
   ],
   "source": [
    "classifier_nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(200, 50), random_state=10)\n",
    "\n",
    "classifier_nn.fit(img_train, digit_train)\n",
    "\n",
    "predict_nn = classifier_nn.predict(img_test)\n",
    "\n",
    "predict_nn\n",
    "\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier_nn, metrics.classification_report(digit_test, predict_nn)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(digit_test, predict_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis: Compare the two classification methods on the MNIST data\n",
    "> Here, the neural network classification method has a slightly higher performance than the svm. The reason for this might be due to the data itself is very sparsly distributed in the data space, and therefore, the support vector might have difficulties separating the two classes. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
